# file: model_api.py

from flask import Flask, request, jsonify
from flask_cors import CORS
from huggingface_hub import InferenceClient
from langdetect import detect
from transformers import pipeline, AutoTokenizer
import torch
import logging
from dotenv import load_dotenv
import os
from datetime import datetime

# --- Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()
API_KEY = os.getenv("HUGGINGFACE_API_KEYDeneme0")
MODEL_NAME = "CohereLabs/aya-expanse-8b"

# --- Logger ayarÄ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("model_api")

# --- Flask app tanÄ±mÄ±
app = Flask(__name__)
CORS(app)

# --- Hugging Face API istemcisi (Cohere Ã¼zerinden)
client = InferenceClient(provider="cohere", api_key=API_KEY)

# --- Tokenizer ayarÄ± (aya-expanse desteklemediÄŸi iÃ§in alternatif)
tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b", use_fast=True)
MAX_TOTAL_TOKENS = 8192

# --- TÃ¼rkÃ§eden Ä°ngilizceye Ã§eviri pipeline'Ä±
translator = pipeline(
    "translation",
    model="Helsinki-NLP/opus-mt-tr-en",
    device=0 if torch.cuda.is_available() else -1
)

# --- QA endpoint
@app.route("/qa", methods=["POST"])
def qa_answer():
    try:
        data = request.get_json()
        question = data.get("question", "").strip()
        contexts = data.get("contexts", [])

        if not question or not contexts:
            return jsonify({"error": "Eksik veri"}), 400

        context_text = "\n\n".join(ctx.get("raw_text", "").strip() for ctx in contexts if ctx.get("raw_text")).strip()
        if not context_text:
            return jsonify({"answer": ""})

        try:
            lang = detect(question)
        except:
            lang = "tr"

        if lang == "tr":
            prompt = (
                "LÃ¼tfen aÅŸaÄŸÄ±daki talimatlara gÃ¶re TÃœRKÃ‡E olarak yanÄ±t veriniz.\n\n"
                f"Belge metni:\n{context_text}\n\n"
                f"KullanÄ±cÄ±nÄ±n sorusu: {question}\n\n"
                "YanÄ±t:"
            )
        else:
            prompt = (
                f"Document:\n{context_text}\n\n"
                f"User question: {question}\n\n"
                "Answer:"
            )

        # ğŸ”¢ GiriÅŸ token'larÄ±nÄ± hesapla
        input_tokens = tokenizer(prompt, return_tensors="pt")["input_ids"].shape[1]
        max_output_tokens = max(100, min(MAX_TOTAL_TOKENS - input_tokens, 2048))  # en fazla 2048 token, en az 100

        logger.info(f"ğŸ“¥ QA isteÄŸi alÄ±ndÄ±.")
        logger.info(f"â“ Soru: {question}")
        logger.info(f"ğŸ“„ Context (ilk 300 karakter): {context_text[:300]}")
        logger.info(f"ğŸ”¢ GiriÅŸ token: {input_tokens}, max output: {max_output_tokens}")

        try:
            response = client.chat_completion(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_output_tokens,
                temperature=0.7
            )
            choices = response.choices
            if not choices:
                logger.warning("â— Model boÅŸ cevap dÃ¶ndÃ¼rdÃ¼.")
                return jsonify({"answer": ""})

            answer = choices[0].message.content.strip()

        except Exception as e:
            logger.exception("âŒ Model yanÄ±t Ã¼retim hatasÄ±:")
            return jsonify({"error": "Model cevabÄ± alÄ±namadÄ±."}), 500

        log_entry = (
            f"\n{'='*80}\n"
            f"ğŸ•’ Zaman: {datetime.utcnow().isoformat()} UTC\n"
            f"â“ Soru:\n{question}\n\n"
            f"ğŸ“„ Context (ilk 500 karakter):\n{context_text[:500]}\n\n"
            f"ğŸ“¦ Prompt:\n{prompt}\n\n"
            f"ğŸ’¡ Cevap:\n{answer}\n"
            f"{'='*80}\n"
        )
        with open("qa_prompt_log.txt", "a", encoding="utf-8") as f:
            f.write(log_entry)

        return jsonify({"answer": answer})

    except Exception as e:
        logger.exception("âŒ QA API hatasÄ±:")
        return jsonify({"error": str(e)}), 500
